Model Restructure

Schema: (Supply_ID, Time_interval, Feature1, Feature2, ... ..., Label)

Idea: Determine time intervals to which, for each corresponding Supply_ID, will have its characteristic features (need to be defined)


Time interval: 
    - First idea: synchronize intervals, as in same dates (Very unlikely to be possible)
    - Second idea: Organize intervals by supply history. In more detail, each interval has the same length and is ordered according to time.
        For example: Each interval represents a week of a supply. First interval is the first week of available data of this supply. Second interval is the second week and so on.
        Each interval will not be synchronized by dates, but will be synchronized by order in which it happened to that supply

        Example dataset:

        Supply_ID, Interval, Feature1, Label

            0       0                   Fraud
            0       1                   Fraud
            0       2                   Fraud
            1       0                   Regular
            1       1                   Regular
            1       2
            2       0
            2       1   
            2       2
            3       0
            3       1   
            3       2

Implementing second idea:
    - Select time interval: Yearly (for now)
    - Select which datasets to use: Works , Consumption, Customer info and Interruptions (Add more later)
    - For each data set group information by supply per year

        Consumption: 
            RangeIndex: 511 entries, 0 to 510
            Data columns (total 6 columns):
            #   Column     Non-Null Count  Dtype  
            ---  ------     --------------  -----  
            0   Supply_ID  511 non-null    object 
            1   meas_ym    511 non-null    int64  
            2   val_mean   511 non-null    float64
            3   val_max    511 non-null    float64
            4   val_min    511 non-null    float64
            5   val_std    511 non-null    float64
            dtypes: float64(4), int64(1), object(1)
            memory usage: 24.1+ KB
            None

            entries per supply: Supply_ID
            SUPPLY001    2
            SUPPLY002    5
            SUPPLY003    5
            SUPPLY004    5
            SUPPLY005    5
                        ..
            SUPPLY096    5
            SUPPLY097    6
            SUPPLY098    6
            SUPPLY099    5
            SUPPLY100    5
            Length: 100, dtype: int64

            max entries: 6
            min entries: 2
            mean entries: 5.11

        Customer info:
            For customer info also group 

            For this dataset I want to do something similar. I still want to group the data by supply and year, but instead of computing some 
            function on this data I want to have a column indicating if in that year the supply was active, terminated, fictitious or mixed (cases where in that year supply was more than only one of these categories).
            I want the end schema to look like: (Supply_ID, Year, Available_power, Status) where status indicates A for active, C for terminated, F for fictitious and M for mixed

            For tomorrow: Decide what to do with null values (set everything to 0), so stats on available power can be computed
            Try both and see how much data is lost
            Null values set to 0

        Works: 
            FELIPE

        Interruptions:
            FELIPE

        Status_words:
            FELIPE

    Merging datasets:
        
        There are clear gaps between datasets. Objective is to keep as much information as possible
         which means filling some features with 0s where data is not available

        How periods will work: Periods won`t be synchronized but will represent the ordered history of that supplly.
        So for example if it is decided 5 periods, for each supply these will be the first 5 years of data logged.

        How to find best number of periods?
        First find supply with the least number of years. (using consumption as a basis)
        In consumption there are supplies with only 2 years and max of 6 years and mean is 5 years.
        Begin by using 6 periods (years) and fill in with 0s supplies that don`t contain enough data.

        Consumption:
        NaN values per column:
            Supply_ID     0
            meas_ym       0
            val_mean     89
            val_max      89
            val_min      89
            val_std      89
            dtype: int64

            Non NaN values per column:
            Supply_ID    600
            meas_ym      600
            val_mean     511
            val_max      511
            val_min      511
            val_std      511


        Now based on the consumption years, extract the same from other datasets.
        
        Customer_info:
        Based on years extracted by consumption data filter rows of corresponding years and add empty rows in the case year not present.
        Now check if dataset correctly Processed.

                NaN values per column:
        Supply_ID                 0
        Year                      0
        mean_available_power    106
        Status                  106
        Status_encoded          106
        dtype: int64

        Non NaN values per column:
        Supply_ID               600
        Year                    600
        mean_available_power    494
        Status                  494
        Status_encoded          494

    Consumption and Customer_info merge:
                NaN values per column:
        Supply_ID                 0
        Year                      0
        val_mean                 94
        val_max                  94
        val_min                  94
        val_std                  94
        mean_available_power    106
        Status                  106
        Status_encoded          106
        dtype: int64

        Non NaN values per column:
        Supply_ID               600
        Year                    600
        val_mean                506
        val_max                 506
        val_min                 506
        val_std                 506
        mean_available_power    494
        Status                  494
        Status_encoded          494
        dtype: int64


    Up next: Process other datasets to make final merge. Then with final merge prepare data to train XGboost model.
    
    Skipped into testing created dataset with consumption and customer info on XGboost:
        - Label the created dataset
        - Pre-processing:
            Handle Nans, handle possible imbalance. 
            Fill NaNs with column mean. Drop rows that contain more than 3 0 values.
        - Create XGboost classifier
 ======= FOR LATER ========
Analyse model and see what improvements can be made:
    - Check label balance
    - splitted correctly
    - Final dataset size 
    - Hyperparameters
    - Dont drop Supply_ID




============== Current status (Phone Notes) ===============

What i tried so far:

Approach 1: Only consumption data
(Supply_ID, mean_Period1, mean_Period2… Label)

Each period contains the mean of consumed energy over the defined period time frame, over a defined number of years. Example: quarter hour average, half hour average, hourly average over 400 days. A more detailed example (hourly average over 400 days): mean of hour 1 over all days, mean of hour 2 over all days… mean of hour 24 over all days, label.

Result: model performed well predicting regular supplies, but horrible predicting fraud.
Diagnosis: Due to high label imbalance (too many regular supplies compared to fraud), model predicted only regular supplies resulting in good accuracy overall but horrible recall. No use of data other than consumption
Notes: Not clear how notion of time is present (sequential?). Each period is present in a different feature instead of having a single period feature and its corresponding features.
Hourly mean over many days likely smooths out fraud behavior. 


Approach 2: Consumption data and available power data used
(Supply_ID, Period, Feature1… Label)

Every supply has 6 periods in which each represents a year of their history. Numbered 1-6 in ascending corresponding year order.
Every feature corresponded to that supply period. Example: Max consumption, Number of works, available power, etc…
The label for each supply corresponded to if that supply in general was fraud or not. Example: Supply_81 was fraudulent, so every period for Supply_81 was fraudulent as well.

Result: Model performed well predicting regular supply periods but did horribly predicting fraudulent periods.
Diagnosis: High label imbalance resulted in the model predicting regular for every period. Even with the addition of features, horrible performance.
Notes: In order to combine available data into the given period, information was lost and gaps (NaN) were present. Also, one year period summarized data likely smooths out fraudulent behavior, making it difficult for model to identify fraudulent periods. Finally, assuming every period of a fraudulent supply is fraudulent is not necessarily correct since a supply might have committed a fraud for only a given time frame of its history.


New approach: Sliding window

(Supply_ID, Feature1, Feature2…, Label)

Define features that for each supply id work as a sliding window. In more detail, define important points in time from the most current day (or not), that these features will represent. Example: 7 days ago, 14 days ago, 30 day ago, 90 days ago and so on. And for each of these time steps have features like max consumption, mean consumption, variance, and so on. The decision on these features will be very important.
In this matter, by using more granular time steps, possible fraudulent patterns are less likely to be smoothed out and, by using the sliding window, we are preserving the aspect of time-series (sequential events) (study to understand further).


Which features to include per time step:
	⁃	variance/std consumption: This will capture any deviations from a “normal” behavior of the time step
	⁃	max consumption:
	⁃	min consumption:

Given the study done on the consumption data set, possible time steps to include as many supplies as possible are:
	⁃	every 7 days
	⁃	every 14 days 
	⁃	every 30 days
	⁃	7/14/30/90/120/150/180…
	⁃	Think of some kind of analysis other than just trying out time steps that will reveal the best split 

After this is done, combining this approach with an unsupervised anomaly detection model could reveal what time step was anamolous and there for possibly fraudulus.
Though new and unlabeled data does not exist, this combination might be useful for reaching the final goal.


===========================

======= Implement sliding window ===========

Schema: (Supply_ID, Feature1, Feature2...  label)

Features:
    Period split: Use 400 days. X by X days. For each X define the following feaures: max, min, mean, std.

Labels:
    Use Label dataset provided

Data:
    - 97 supplies. Fraud: 10. Regular: 87
    - 40 features. mean, max, min and sd for every 10 days per supply

Model: model = XGBClassifier(
        objective='binary:logistic',  # For binary classification
        max_depth=6,                  # Maximum depth of trees
        learning_rate=0.1,            # Learning rate (eta)
        n_estimators=100,             # Number of boosting rounds
        random_state=42,               # Random seed for reproducibility
        scale_pos_weight = scale_pos_weight # For imbalanced dataset
    )

Training:
    - Split: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    - Train: 77 supplies, 8 fraud, 69 regular
    - Test: 20 supplies, 2 fraud, 18 regular

Performance:
    - Classification Report:
                precision    recall  f1-score   support

            0       1.00      0.94      0.97        18
            1       0.67      1.00      0.80         2

        accuracy                           0.95        20
    macro avg       0.83      0.97      0.89        20
    weighted avg       0.97      0.95      0.95        20

    - Confusion Matrix:
            [[17  1]
            [ 0  2]]

